## Language model, Deep Learning 공부

### 위키피디아로 해보기

[위키피디아 한국어 데이터셋](https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4_%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C)

여기서 데이터셋을 다운로드 받았음

[WikiExtractor](https://github.com/attardi/wikiextractor)
WikiExtractor로 위키 데이터셋을 txt로 바꿈

-> 노트북 터질듯 이건 포기

### Neural Networks: Zero to Hero

- [X] [The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0?si=wzwuY7FB-2VVHghH)
- [X] [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2)
- [X] [Building makemore Part 2: MLP](https://youtu.be/TCH_1BHY58I?si=a5eiczYMcWA7Kijg)
- [ ] [Building makemore Part 3: Activations & Gradients, BatchNorm](https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4)
- [ ] [Building makemore Part 4: Becoming a Backdrop Ninja]()
- [ ] [Building makemore Part 5: Building a WaveNet]()
- [ ] [Let's build GPT: from scratch, in code, spelled out]()
- [ ] [State of GPT | BRK216HFS]()
- [ ] [Let's build the GPT Tokenizer]()
- [ ] [Let's reproduce GPT-2 (124M)]()
- [ ] [llm.c]()




먼저 카나나 finetuning 해보기


### Paper
[Attention is All you Need](https://arxiv.org/abs/1706.03762)